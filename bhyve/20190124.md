# Attendees

- Patrick Mooney
- Rod Grimes
- Alexandru Elisei
- Michael Dexter
- Sergiu Weisz

# Notes

Continuing from last time:

**Patrick Mooney:** Driver resource exclusions.  Patrick has posted a
code review on [Joyent’s repo](https://cr.joyent.us/#/c/5391/).
vmm_dev file.  Open to getting others to review it.  Rod is happy to
review it.

**Michael Dexter:** bhyvecon Tokyo is planned for March 20th, pending
venue confirmation.

**Rod Grimes:** Reviews up in phabricator, are you seeing them
Patrick?  Patrick confirms.

**Patrick:** Please do include Patrick on reviews.freebsd.org tickets.
Is subscribed to the “bhyve” umbrella but two notifications are
fine.  (Phab may dedupe)

Known to be missing and waiting on Rod’s VM_MAXCPU work: A check when
acquiring the write lock to query the number of vCPUs before locking
them.  Must re-check what to lock.  Use a “lock all”? Patrick and Rod
agree on the approach but must document the resulting behavior.

**Rod** (post call documenting): The agreed protocol is that if
vm->maxcpus is 0, acquire the lock, and check that maxcpus is still 0,
if not you have to retry else you have the lock, if vm->maxcpus is not
0 simply acquire the lock by locking all the vCPU’s.

Rod has four patches in reviews.freebsd.org to remove the VM_MAXCPU
constant from the kernel and userland. There are three areas that need
to be addressed, including MADT bugs, sizing constants that depend on
the vCPU numbers.  Those need to be aware of it.  Some data
restructuring for unwrapping two-dimensional arrays to avoid pointer
arithmetic.  Dexter ran tests that reached 65 vCPUs.  Missed a
structure in SMBIOS.  (Like MADT table) Should have a patch shortly.

Feedback welcome:
https://reviews.freebsd.org/D18815
https://reviews.freebsd.org/D18816
https://reviews.freebsd.org/D18846
https://reviews.freebsd.org/D18755

**Rod:** This is expected to grow to about 7 patches, missing are
SMBios table overflow at 65 vCPU just found by Michael Dester’s
testing, malloc’ing of per vCPU datastructs, and handling of nulls
when per vCPU data has not been allocated yet (lazy allocation).  Note:
This will thrash your TLB.

TO DO: Test various CPU topologies with high vCPU counts when this is
complete. (May hit limits of 256 sockets etc. and “the APIC may get
weird”.)

Wish list: A UEFI ROM that behaves more like QEMU and dynamically
builds an ACPI table. Some code is there.  UEFI ROM replaces tables
when built... Update to 2017 UEFI?

To play with bhyve in Illumos, use SmartOS or OmniOS CE Patrick can
provide tips.  Can it NFS boot?  Designed for a local disk.  iSCSI
would be tricky.  Designed for local resources, rather than iSCSI or
SAN.  KVM also uses Tianocore?  OVMF package with QEMU hooks?  Hope to
expose the bhyve interfaces to use the OVMF package.  Would help
staying current.  Tycho is the one to ask given his core Tianocore
work.

**Alexandru Elisei:** A VM with more than 256 CPUs can be useful for
testing small kernels like KVM-unit-tests.  The ARM code is not SMP
aware yet.  One vCPU only.

**Rod:** The plan for VM_MAXCPU removal is to unrestrict the limit on
vCPU such that we start to run into the other real limits like the
CPUID topology instruction bit field widths, APIC addressing schemes,
etc.

**Michael** will share (has shared) his [STUDENT](http://bhyve.org/STUDENT)
kernel and Build Option Survey findings and Alex will share his ARM64
cross-building scripts.  The STUDENT kernel is a truly stripped
MINIMAL kernel in that it is just enough to get it to compile and boot
on a very small set of hardware (bhyve guest).

[Ticket to review](https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=229852):
“bhyve: IOMMU (Intel VTd) PCI passthrough attempt locks up some
systems” for PCI-Passthrough failing for a multi IOMMUs situation
(multi-socket system) We treat the IOMMUs as equivalent, which is
wrong.  The patch seems well tested by those having issues, it would
be good to have the patch tested by others to make sure we do not
break working systems.  Test the patch on single sockets!

**Sergiu Weisz** is working on [QCOW support](https://github.com/FreeBSD-UPB/freebsd/tree/projects/bhyve_libvdsk).

**Rod:** People are requesting ESXi cold/offline migration to bhyve.
Rod has some info that could go on a Wiki.  You can use VMware for the
conversion.  No need to use the VirtualBox tool.  Few people seem to
be aware of the vmkfstool on ESXi that allows you to convert your
vmdk’s to non sparse “eager zero” VMDK’s which are actually raw disk
images, the .vmdk file is a text description of the raw data stored in
the *-flat.vmdk file.

**Patrick:** Copyright attribution issues remain. VirtIO ring size
fix, PIT emulation, potentially parallel VMM support/HVM Abstraction
(KVM/bhyve or VBox/bhyve at the same time)… Can’t upstream at the
moment. By nature, fixes will not always reach the 25% change
threshold.  KAME IPv6 project may have followed this model.
University of California, Berkeley realized early on that not every
change warrants credit given how large the list in every file would
get.

One proposal: Create a bhyve CONTRIBUTORS file and place in each file
a copyright notice that says “Copyright (x-y blanket years) the
CONTRIBUTORS to bhyve (path to file))”

**Q:** How is OpenZFS handling this?

Rod will reach out to John and Core.
